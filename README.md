# ***Reinforcement Learning***

- ***Feedback-based machine learning approach.***
- ***An `agent` interacts with an `environment` with an objective to `maximize` its total `reward`.***
 
***The agent takes an action based on the environment state and the environment returns the reward and the next state. The agent learns from trial and error, initially taking random actions and over time identifying the actions that lead to long-term rewards.***   

<img src="https://github.com/Rafiqul-Islam12/Reinforcement-Learning-Algorithm/blob/main/images/img01.png" width="600">  

- ***`Agent`: Learns and makes decisions.***
- ***`Environment`: The external system the agent interacts with.***
- ***`Action`: What the agent does.***
- ***`State`: The current situation of the agent.***
- ***`Reward`: Feedback from the environment.***
  
### ***`Feedback`***:
- ***Correct actions receive positive feedback.***
- ***Incorrect actions receive negative feedback or penalties.***

---
### ***We can break down reinforcement learning into five simple steps:***   
- ***The agent is at state zero in an environment.***
- ***It will take an action based on a specific strategy.***
- ***It will receive a reward or punishment based on that action.***
- ***By learning from previous moves and optimizing the strategy.*** 
- ***The process will repeat until an optimal strategy is found.***

---
# ***Core Components***   
## üîπ***Policy (œÄ)***  
- ***Policy ‡¶Æ‡¶æ‡¶®‡ßá ‡¶π‡¶≤‡ßã agent-‡¶è‡¶∞ `decision-making rule`***  
  ***‡¶è‡¶á state ‡¶è ‡¶•‡¶æ‡¶ï‡¶≤‡ßá ‡¶ï‡ßã‡¶® action ‡¶®‡¶ø‡¶¨‡ßã ‚Äî ‡¶è‡¶á rule ‡¶ü‡¶æ‡¶á Policy.***

***Math ‡¶è, `œÄ(s)=a`***    
***‡¶Æ‡¶æ‡¶®‡ßá, state s ‡¶è ‡¶ó‡ßá‡¶≤‡ßá action a ‡¶®‡ßá‡¶¨‡ßá‡•§***   

- ***Policy ‡¶ï‡ßá‡¶® ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞?***   
  ***Reinforcement Learning / MDP ‡¶§‡ßá agent ‡¶∏‡¶¨‡¶∏‡¶Æ‡ßü ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶ï‡¶∞‡ßá: ‚Äú‡¶è‡¶ñ‡¶® ‡¶Ü‡¶Æ‡¶ø ‡¶ï‡ßÄ ‡¶ï‡¶∞‡¶¨‡ßã?‚Äù***  
  ***‡¶è‡¶á ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡ßá‡¶∞ ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¶‡ßá‡ßü Policy‡•§***  

***Policy ‡¶π‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá:***  
***`Deterministic` ‚Üí always same action***  
***`Stochastic` ‚Üí probability ‡¶¶‡¶ø‡ßü‡ßá action ‡¶®‡ßá‡ßü***  

***Goal:***  
***best policy (œÄ*) ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ, ‡¶Ø‡¶æ‡¶§‡ßá total reward maximum ‡¶π‡ßü‡•§***  

## üîπ***Value Function***  
- ***‡¶è‡¶ü‡¶æ ‡¶¨‡¶≤‡ßá ‡¶¶‡ßá‡ßü ‡¶ï‡ßã‡¶® state ‡¶¨‡¶æ action ‡¶ï‡¶§‡¶ü‡¶æ ‡¶≠‡¶æ‡¶≤‡ßã‡•§***   

***V(s) ‚Üí ‡¶è‡¶á state ‡¶è ‡¶•‡¶æ‡¶ï‡¶≤‡ßá future ‡¶è ‡¶Æ‡ßã‡¶ü reward ‡¶ï‡¶§ ‡¶™‡¶æ‡¶ì‡ßü‡¶æ ‡¶Ø‡¶æ‡¶¨‡ßá***   
***Q(s, a) ‚Üí ‡¶è‡¶á state ‡¶è ‡¶è‡¶á action ‡¶®‡¶ø‡¶≤‡ßá ‡¶ï‡¶§ ‡¶≠‡¶æ‡¶≤‡ßã ‡¶π‡¶¨‡ßá***   
***‡¶è‡¶á Q-value ‡¶¶‡¶ø‡ßü‡ßá‡¶á Q-Learning ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá‡•§***   

## üîπ***Episode*** 
- ***‚ÄúA complete sequence from initial state to terminal state.‚Äù***    
  ***Episode ‡¶π‡¶≤‡ßã, Start state ‡¶•‡ßá‡¶ï‡ßá ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶ï‡¶∞‡ßá terminal state ‡¶™‡¶∞‡ßç‡¶Ø‡¶®‡ßç‡¶§ complete journey.***

## üîπ***Iteration***  
- ***Algorithm ‡¶è‡¶∞ ‡¶è‡¶ï‡¶¨‡¶æ‡¶∞ update ‡¶π‡¶ì‡ßü‡¶æ***

***Example:***   
***Q-table ‡¶è‡¶ï‡¶¨‡¶æ‡¶∞ update ‚Üí 1 iteration***  
***Value iteration ‡¶è ‡¶è‡¶ï‡¶¨‡¶æ‡¶∞ Bellman update ‚Üí 1 iteration***   
***Iteration ‚â† Episode***   

## üîπ***Exploration vs Exploitation*** 
***‚ÄúExploration tries new actions, exploitation uses known best actions.‚Äù***   
- ***Exploration (‡¶®‡¶§‡ßÅ‡¶® ‡¶ú‡¶ø‡¶®‡¶ø‡¶∏ try ‡¶ï‡¶∞‡¶æ): Agent ‡¶®‡¶§‡ßÅ‡¶® action try ‡¶ï‡¶∞‡ßá, ‡¶Ø‡ßá‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ outcome ‡¶∏‡ßá ‡¶†‡¶ø‡¶ï ‡¶ú‡¶æ‡¶®‡ßá ‡¶®‡¶æ‡•§***  
- ***Exploitation (‡¶ú‡¶æ‡¶®‡¶æ ‡¶≠‡¶æ‡¶≤‡ßã ‡¶ú‡¶ø‡¶®‡¶ø‡¶∏ use ‡¶ï‡¶∞‡¶æ): Agent ‡¶Ø‡ßá‡¶ü‡¶æ ‡¶Ü‡¶ó‡ßá ‡¶•‡ßá‡¶ï‡ßá best ‡¶ú‡¶æ‡¶®‡ßá, ‡¶∏‡ßá‡¶ü‡¶æ‡¶á ‡¶ï‡¶∞‡ßá‡•§***

## üîπ***Convergence*** 
***Training ‡¶ö‡¶≤‡¶§‡ßá ‡¶ö‡¶≤‡¶§‡ßá ‡¶Ø‡¶ñ‡¶®:***   
***Q-values ‡¶Ü‡¶∞ change ‡¶π‡ßü ‡¶®‡¶æ, Policy stable ‡¶π‡ßü‡ßá ‡¶Ø‡¶æ‡ßü***   
***‡¶§‡¶ñ‡¶® ‡¶¨‡¶≤‡¶ø algorithm converge ‡¶ï‡¶∞‡ßá‡¶õ‡ßá‡•§***   

---
# üìå ***Policy-based Method vs Value-based Method***   
## ***Value-based Method***    
***Agent ‡¶Ü‡¶ó‡ßá ‡¶∂‡ßá‡¶ñ‡ßá: `‚Äú‡¶è‡¶á state ‡¶¨‡¶æ action ‡¶ï‡¶§‡¶ü‡¶æ ‡¶≠‡¶æ‡¶≤‡ßã?‚Äù`***  
***‡¶Æ‡¶æ‡¶®‡ßá Value Function ‡¶∂‡ßá‡¶ñ‡ßá, ‡¶§‡¶æ‡¶∞‡¶™‡¶∞ ‡¶∏‡ßá‡¶ñ‡¶æ‡¶® ‡¶•‡ßá‡¶ï‡ßá policy ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßá***   
***Indirectly policy ‡¶∂‡ßá‡¶ñ‡ßá***   

***Algorithms***   
- ***Q-Learning***
- ***SARSA***
- ***Deep Q Network (DQN)***

## ***Policy-based Method***  
***Agent directly policy ‡¶∂‡ßá‡¶ñ‡ßá‡•§***  
***‡¶Æ‡¶æ‡¶®‡ßá agent ‡¶®‡¶ø‡¶ú‡ßá‡¶á ‡¶∂‡ßá‡¶ñ‡ßá: `‚Äú‡¶è‡¶á state ‡¶è ‡¶ï‡ßã‡¶® action ‡¶®‡¶ø‡¶≤‡ßá ‡¶≠‡¶æ‡¶≤‡ßã‚Äù`***    
***No value table required***    

***Algorithms***   
- ***Policy Gradient***  
- ***REINFORCE***   
- ***Actor-Critic (hybrid)***   

---
# ***Markov Decision Process (MDP)***  
- ***It is a `decision making framework`.***    
  ***‡¶Ø‡ßá‡¶ü‡¶æ Reinforcement Learning-‡¶è use ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡•§ ‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶è‡¶ï‡¶ü‡¶æ agent ‡¶•‡¶æ‡¶ï‡ßá, ‡¶∏‡ßá ‡¶è‡¶ï‡¶ü‡¶æ environment-‡¶è‡¶∞ ‡¶≠‡¶ø‡¶§‡¶∞‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá, action ‡¶®‡ßá‡ßü, ‡¶Ü‡¶∞ reward ‡¶™‡¶æ‡ßü‡•§***

***MDP ‡¶ï‡ßá ‡¶Ü‡¶Æ‡¶∞‡¶æ normally ‡¶è‡¶á‡¶≠‡¶æ‡¶¨‡ßá define ‡¶ï‡¶∞‡¶ø: `(S, A, P, R, Œ≥)`***   
- ***`S = States`***  
  ***State ‡¶Æ‡¶æ‡¶®‡ßá agent ‡¶è‡¶ñ‡¶® ‡¶ï‡ßã‡¶® situation-‡¶è ‡¶Ü‡¶õ‡ßá‡•§***  
  ***‡¶Ø‡ßá‡¶Æ‡¶®, ‡¶è‡¶ï‡¶ü‡¶æ game ‡¶è player ‡¶ï‡ßã‡¶® position-‡¶è ‡¶Ü‡¶õ‡ßá, ‡¶Ö‡¶•‡¶¨‡¶æ car ‡¶ï‡ßã‡¶® ‡¶ú‡¶æ‡ßü‡¶ó‡¶æ‡ßü ‡¶Ü‡¶õ‡ßá‚Äî‡¶è‡¶ó‡ßÅ‡¶≤‡ßã state***  

- ***`A = Actions`***  
  ***Agent state ‡¶¶‡ßá‡¶ñ‡ßá action ‡¶®‡ßá‡ßü‡•§***   
  ***‡¶Ø‡ßá‡¶Æ‡¶®: left, right, accelerate, brake, up, down ‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø‡•§***
  
- ***`P = Transition Probability`***  
  ***P(s‚Ä≤ | s, a) ‡¶Æ‡¶æ‡¶®‡ßá, current state s ‡¶•‡ßá‡¶ï‡ßá action a ‡¶®‡¶ø‡¶≤‡ßá next state s‚Ä≤ ‡¶è ‡¶Ø‡¶æ‡¶ì‡ßü‡¶æ‡¶∞ probability ‡¶ï‡¶§‡•§***  
  ***‡¶∏‡¶¨ environment deterministic ‡¶®‡¶æ, ‡¶§‡¶æ‡¶á probability ‡¶≤‡¶æ‡¶ó‡ßá‡•§***  

- ***`R = Reward`***  
  ***Agent action ‡¶®‡ßá‡ßü‡¶æ‡¶∞ ‡¶™‡¶∞‡ßá environment ‡¶§‡¶æ‡¶ï‡ßá ‡¶è‡¶ï‡¶ü‡¶æ reward ‡¶¶‡ßá‡ßü‡•§***  
  ***‡¶≠‡¶æ‡¶≤‡ßã ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡¶≤‡ßá positive reward, ‡¶ñ‡¶æ‡¶∞‡¶æ‡¶™ ‡¶ï‡¶∞‡¶≤‡ßá negative reward‡•§***  
  ***‡¶Ø‡ßá‡¶Æ‡¶®: goal ‡¶è ‡¶™‡ßå‡¶Å‡¶õ‡¶æ‡¶≤‡ßá +10, ‡¶¶‡ßá‡ßü‡¶æ‡¶≤‡ßá ‡¶ß‡¶æ‡¶ï‡ßç‡¶ï‡¶æ ‡¶¶‡¶ø‡¶≤‡ßá ‚àí5‡•§***  

- ***`Œ≥ (Gamma) = Discount Factor`***  
  ***Future reward ‡¶ï‡¶§‡¶ü‡¶æ important ‡¶∏‡ßá‡¶ü‡¶æ control ‡¶ï‡¶∞‡ßá‡•§***   
  ***Œ≥ ‚âà 1 ‡¶π‡¶≤‡ßá ‚Üí long-term reward ‡¶¨‡ßá‡¶∂‡¶ø important***   
  ***Œ≥ ‚âà 0 ‡¶π‡¶≤‡ßá ‚Üí immediate reward ‡¶¨‡ßá‡¶∂‡¶ø important***   

---
## ***Markov Property***   
***Future ‡¶∂‡ßÅ‡¶ß‡ßÅ current state + current action ‡¶è‡¶∞ ‡¶â‡¶™‡¶∞ depend ‡¶ï‡¶∞‡ßá, past history ‡¶è‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶®‡¶æ‡•§***   
***Math ‡¶è ‡¶≤‡¶ø‡¶ñ‡¶≤‡ßá:*** ***`P(s‚Çú‚Çä‚ÇÅ | s‚Çú, a‚Çú)`***    
***‡¶Æ‡¶æ‡¶®‡ßá agent ‡¶ï‡ßá ‡¶Ü‡¶ó‡ßá‡¶∞ ‡¶∏‡¶¨ ‡¶Æ‡¶®‡ßá ‡¶∞‡¶æ‡¶ñ‡¶§‡ßá ‡¶π‡ßü ‡¶®‡¶æ, ‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶è‡¶ñ‡¶® ‡¶ï‡ßã‡¶•‡¶æ‡ßü ‡¶Ü‡¶õ‡ßá ‡¶∏‡ßá‡¶ü‡¶æ‡¶á ‡¶Ø‡¶•‡ßá‡¶∑‡ßç‡¶ü‡•§***   

---
